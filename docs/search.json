[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bowen Yao",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Project",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nBowen Yao\n\n\nApr 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nYour Name\n\n\nMay 6, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/hw1_questions.html",
    "href": "blog/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the experiment, more than 50,000 previous donors were randomly assigned to receive one of several direct-mail solicitations. These solicitations varied in three main dimensions: - Whether a matching grant was offered, and at what ratio ($1:$1, $2:$1, or $3:$1), - The maximum threshold for matching (e.g., $25,000, $50,000, or $100,000), - The suggested donation amount on the reply card, based on prior giving history.\nThe central hypothesis was that offering a matching grant would increase both the likelihood of giving and the total donation amount. The authors also explored whether larger match ratios (e.g., $3:$1) would be more effective than smaller ones.\nImportantly, because of the randomized assignment of treatments, the experiment allows for clean causal inference on the effect of each fundraising strategy.\nOne notable finding from their analysis was that offering any match significantly increased donations, but increasing the match ratio beyond $1:$1 had no additional benefit. Moreover, they found that responses varied by political context—the treatment effects were stronger in U.S. states that voted for George W. Bush in the 2004 presidential election.\nThis project seeks to replicate and visualize these key findings using the original dataset and modern statistical tools."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#introduction",
    "href": "blog/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the experiment, more than 50,000 previous donors were randomly assigned to receive one of several direct-mail solicitations. These solicitations varied in three main dimensions: - Whether a matching grant was offered, and at what ratio ($1:$1, $2:$1, or $3:$1), - The maximum threshold for matching (e.g., $25,000, $50,000, or $100,000), - The suggested donation amount on the reply card, based on prior giving history.\nThe central hypothesis was that offering a matching grant would increase both the likelihood of giving and the total donation amount. The authors also explored whether larger match ratios (e.g., $3:$1) would be more effective than smaller ones.\nImportantly, because of the randomized assignment of treatments, the experiment allows for clean causal inference on the effect of each fundraising strategy.\nOne notable finding from their analysis was that offering any match significantly increased donations, but increasing the match ratio beyond $1:$1 had no additional benefit. Moreover, they found that responses varied by political context—the treatment effects were stronger in U.S. states that voted for George W. Bush in the 2004 presidential election.\nThis project seeks to replicate and visualize these key findings using the original dataset and modern statistical tools."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#data",
    "href": "blog/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ndf.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\ndf.shape\n\n(50083, 51)\n\n\n\ndf.dtypes\n\ntreatment                 int8\ncontrol                   int8\nratio                 category\nratio2                    int8\nratio3                    int8\nsize                  category\nsize25                    int8\nsize50                    int8\nsize100                   int8\nsizeno                    int8\nask                   category\naskd1                     int8\naskd2                     int8\naskd3                     int8\nask1                     int16\nask2                     int16\nask3                     int16\namount                 float32\ngave                      int8\namountchange           float32\nhpa                    float32\nltmedmra                  int8\nfreq                     int16\nyears                  float64\nyear5                     int8\nmrm2                   float64\ndormant                   int8\nfemale                 float64\ncouple                 float64\nstate50one                int8\nnonlit                 float64\ncases                  float64\nstatecnt               float32\nstateresponse          float32\nstateresponset         float32\nstateresponsec         float32\nstateresponsetminc     float32\nperbush                float32\nclose25                float64\nred0                   float64\nblue0                  float64\nredcty                 float64\nbluecty                float64\npwhite                 float32\npblack                 float32\npage18_39              float32\nave_hh_sz              float32\nmedian_hhincome        float64\npowner                 float32\npsch_atlstba           float32\npop_propurban          float32\ndtype: object\n\n\n\ndf.isnull().sum()\n\ntreatment                0\ncontrol                  0\nratio                    0\nratio2                   0\nratio3                   0\nsize                     0\nsize25                   0\nsize50                   0\nsize100                  0\nsizeno                   0\nask                      0\naskd1                    0\naskd2                    0\naskd3                    0\nask1                     0\nask2                     0\nask3                     0\namount                   0\ngave                     0\namountchange             0\nhpa                      0\nltmedmra                 0\nfreq                     0\nyears                    1\nyear5                    0\nmrm2                     1\ndormant                  0\nfemale                1111\ncouple                1148\nstate50one               0\nnonlit                 452\ncases                  452\nstatecnt                 0\nstateresponse            0\nstateresponset           0\nstateresponsec           3\nstateresponsetminc       3\nperbush                 35\nclose25                 35\nred0                    35\nblue0                   35\nredcty                 105\nbluecty                105\npwhite                1866\npblack                2036\npage18_39             1866\nave_hh_sz             1862\nmedian_hhincome       1874\npowner                1869\npsch_atlstba          1868\npop_propurban         1866\ndtype: int64\n\n\nThe dataset contains 50,083 observations and 51 variables. Each row represents a unique individual who received one of the fundraising letters in the Karlan and List (2007) field experiment.\nThe variables include: - Treatment indicators (treatment, control, ratio, size, etc.) describing the type and details of the fundraising letter received. - Suggested donation values and historical giving behavior (ask, ask1, hpa, etc.). - Outcome variables, such as whether a donation was made (gave) and the amount given (amount). - Demographic and geographic variables, such as gender (female), income (median_hhincome), and urbanization (pop_propurban).\nMost variables are either integer-coded dummy variables or numeric, with some categorical fields (e.g., ratio, size, ask).\nThe dataset is overall very clean: - The vast majority of columns have no missing values. - A few variables, primarily those related to demographics (e.g., female, couple, pwhite, median_hhincome), contain some missing values—up to around 3.7% missing in the worst case (1,874 out of 50,083). - These missing values are mostly concentrated in the geographic/demographic features and do not affect the treatment or outcome variables.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nfrom scipy import stats\nfrom tabulate import tabulate\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\nvariables_to_test = ['mrm2', 'freq', 'female']\n\nresults = []\n\nfor var in variables_to_test:\n    subset = df[['treatment', var]].dropna()\n\n    treat = subset[subset['treatment'] == 1][var]\n    control = subset[subset['treatment'] == 0][var]\n\n    x1, x2 = treat.mean(), control.mean()\n    s1, s2 = treat.std(), control.std()\n    n1, n2 = treat.count(), control.count()\n    t_stat = (x1 - x2) / np.sqrt((s1**2)/n1 + (s2**2)/n2)\n    dfree = min(n1, n2) - 1\n    p_val_ttest = 2 * (1 - stats.t.cdf(np.abs(t_stat), dfree))\n\n    model = smf.ols(f\"{var} ~ treatment\", data=subset).fit()\n    reg = model.summary2().tables[1].loc['treatment']\n\n    results.append([\n        var,\n        round(x1, 3), round(x2, 3),\n        round(t_stat, 3), round(p_val_ttest, 4),\n        round(reg['Coef.'], 3), round(reg['Std.Err.'], 3),\n        round(reg['t'], 3), round(reg['P&gt;|t|'], 4)\n    ])\n\nheaders = [\"Variable\", \"Mean (Treat)\", \"Mean (Control)\",\n           \"T-stat (manual)\", \"P-val (t-test)\",\n           \"Coef (reg)\", \"Std Err\", \"T-stat (reg)\", \"P-val (reg)\"]\n\nprint(tabulate(results, headers=headers, tablefmt=\"github\"))\n\n| Variable   |   Mean (Treat) |   Mean (Control) |   T-stat (manual) |   P-val (t-test) |   Coef (reg) |   Std Err |   T-stat (reg) |   P-val (reg) |\n|------------|----------------|------------------|-------------------|------------------|--------------|-----------|----------------|---------------|\n| mrm2       |         13.012 |           12.998 |             0.12  |           0.9049 |        0.014 |     0.115 |          0.119 |        0.9049 |\n| freq       |          8.035 |            8.047 |            -0.111 |           0.9117 |       -0.012 |     0.108 |         -0.111 |        0.9117 |\n| female     |          0.275 |            0.283 |            -1.754 |           0.0795 |       -0.008 |     0.004 |         -1.758 |        0.0787 |\n\n\nAs a robustness check on the validity of the randomization procedure, I performed balance tests comparing the treatment and control groups across three pre-treatment variables that are not outcome variables:\n\nmrm2 (months since last donation),\n\nfreq (number of prior donations),\n\nfemale (gender indicator).\n\nFor each variable, I performed both a manual t-test and a simple linear regression with the variable as the outcome and treatment as the predictor.\nWe can see that none of the variables exhibit statistically significant differences between the treatment and control groups at the 95% confidence level. All t-statistics are small in absolute value and p-values are above 0.05. The manual t-tests and regression results match perfectly, as expected\n\n\nInterpretation & Comparison with Table 1 in the Paper\nThese findings replicate what is shown in Table 1 of Karlan and List (2007), which also shows minimal differences between treatment and control groups across various pre-treatment characteristics. Table 1 is included in the original paper as a demonstration of successful randomization. If randomization is properly implemented, the only expected differences between groups should be due to chance — not systematic differences in characteristics.\nOur results confirm this. Variables such as months since last donation, number of prior donations, and gender are all statistically balanced across groups. This supports the internal validity of the experiment and allows us to interpret any differences in donation outcomes as causal effects of the treatment rather than artifacts of initial imbalance."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#experimental-results",
    "href": "blog/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ndonation_rate = df.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ndonation_rate[\"Group\"] = donation_rate[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(6, 4))\n\ncolors = [\"#4C72B0\", \"#55A868\"] \nsns.barplot(x=\"Group\", y=\"gave\", data=donation_rate, palette=colors)\nplt.ylim(0, 0.03)\nplt.ylabel(\"Proportion Donated\", fontsize=12)\nplt.title(\"Donation Rate by Treatment Group\", fontsize=14)\nplt.xlabel(\"\")\nplt.xticks(fontsize=11)\nplt.yticks(fontsize=11)\nplt.tight_layout()\n\nplt.show()\n\nC:\\Users\\25645\\AppData\\Local\\Temp\\ipykernel_28864\\3870666356.py:13: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\nThe bar plot above illustrates the proportion of individuals who donated in each group. The treatment group, who received a matching offer, shows a slightly higher donation rate than the control group.\n\nimport numpy as np\nimport statsmodels.formula.api as smf\nfrom scipy import stats\ndf_gave = df[['treatment', 'gave']].dropna()\n\ntreat = df_gave[df_gave['treatment'] == 1]['gave']\ncontrol = df_gave[df_gave['treatment'] == 0]['gave']\n\nx1, x2 = treat.mean(), control.mean()\ns1, s2 = treat.std(), control.std()\nn1, n2 = treat.count(), control.count()\nt_stat = (x1 - x2) / np.sqrt((s1**2)/n1 + (s2**2)/n2)\ndfree = min(n1, n2) - 1\np_val_ttest = 2 * (1 - stats.t.cdf(np.abs(t_stat), dfree))\n\nmodel = smf.ols(\"gave ~ treatment\", data=df_gave).fit()\nreg_result = model.summary2().tables[1].loc['treatment']\n\nprint(f\"Manual t-test: t = {t_stat:.3f}, p = {p_val_ttest:.4f}\")\nprint(f\"Regression: Coef = {reg_result['Coef.']:.4f}, Std Err = {reg_result['Std.Err.']:.4f}, t = {reg_result['t']:.3f}, p = {reg_result['P&gt;|t|']:.4f}\")\n\nManual t-test: t = 3.209, p = 0.0013\nRegression: Coef = 0.0042, Std Err = 0.0013, t = 3.101, p = 0.0019\n\n\nThe t-test and regression results show that the treatment group had a donation rate approximately 0.42 percentage points higher than the control group, which aligns closely with the difference reported in Table 2A Panel A of the original study (2.2% vs. 1.8%).\nOur t-statistic (~3.2) and p-value (0.0013) indicate that this difference is statistically significant at well below the 1% level. The regression confirms this with a nearly identical coefficient and standard error.\n\n\nInterpretation\nEven though the difference in donation rates might seem small numerically, it is statistically meaningful and practically important in the context of large-scale fundraising. The key insight is that human generosity is not fixed — it can be influenced by how giving opportunities are presented.\nThe presence of a matching grant doesn’t just increase the value of each donation; it also creates a psychological incentive. People may feel that their impact is greater when a match is involved, or that someone else is “joining them” in the cause. This social framing makes them more likely to act. This finding has broad implications for nonprofit fundraising and behavioral economics more generally: well-designed, low-cost incentives can powerfully nudge human behavior.\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ndf_probit = df[['gave', 'treatment']].dropna()\n\nprobit_model = smf.probit(\"gave ~ treatment\", data=df_probit).fit()\n\nmarginal_effects = probit_model.get_margeff()\nmarginal_table = marginal_effects.summary_frame()\noutput = marginal_table[['dy/dx', 'Std. Err.', 'z', 'Pr(&gt;|z|)']]\noutput.columns = ['Marginal Effect', 'Std. Error', 'z', 'p-value']\nfrom tabulate import tabulate\nprint(tabulate(output, headers='keys', tablefmt='github'))\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n|           |   Marginal Effect |   Std. Error |       z |    p-value |\n|-----------|-------------------|--------------|---------|------------|\n| treatment |        0.00431321 |   0.00138938 | 3.10442 | 0.00190653 |\n\n\nThis result confirms the finding in Table 3, Column 1 of Karlan and List (2007): assignment to the treatment group (i.e., receiving a matching donation offer) increases the probability of donating by approximately 0.4 percentage points, and the effect is statistically significant at the 1% level.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nimport pandas as pd\nimport scipy.stats as stats\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ndf_match = df[df['treatment'] == 1]\n\ngave_1to1 = df_match[(df_match['ratio2'] == 0) & (df_match['ratio3'] == 0)]['gave'].dropna()\ngave_2to1 = df_match[df_match['ratio2'] == 1]['gave'].dropna()\ngave_3to1 = df_match[df_match['ratio3'] == 1]['gave'].dropna()\n\ntstat_12, pval_12 = stats.ttest_ind(gave_1to1, gave_2to1)\ntstat_13, pval_13 = stats.ttest_ind(gave_1to1, gave_3to1)\ntstat_23, pval_23 = stats.ttest_ind(gave_2to1, gave_3to1)\n\nprint(\"1:1 vs 2:1 (within treatment) --&gt; t = {:.3f}, p = {:.4f}\".format(tstat_12, pval_12))\nprint(\"1:1 vs 3:1 (within treatment) --&gt; t = {:.3f}, p = {:.4f}\".format(tstat_13, pval_13))\nprint(\"2:1 vs 3:1 (within treatment) --&gt; t = {:.3f}, p = {:.4f}\".format(tstat_23, pval_23))\n\n1:1 vs 2:1 (within treatment) --&gt; t = -0.965, p = 0.3345\n1:1 vs 3:1 (within treatment) --&gt; t = -1.015, p = 0.3101\n2:1 vs 3:1 (within treatment) --&gt; t = -0.050, p = 0.9600\n\n\nTo test the claim made by the authors that “larger match ratios… have no additional impact” (page 8), I conducted a series of t-tests comparing donation rates between different match ratios. The output from the code support the authors’ observation that although donation rates vary slightly across match ratios, these differences are not statistically meaningful. The numbers alone may “suggest” a higher rate under 2:1 matches, but our analysis shows that this is likely due to random variation rather than a true treatment effect.\n\nimport pandas as pd\nimport scipy.stats as stats\nimport statsmodels.formula.api as smf\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf_match = df[df['treatment'] == 1]\n\ndf_ratio1 = df_match[(df_match['ratio2'] == 0) & (df_match['ratio3'] == 0)]\ndf_ratio2 = df_match[df_match['ratio2'] == 1]\ndf_ratio3 = df_match[df_match['ratio3'] == 1]\n\nmodel1 = smf.ols('gave ~ 1', data=df_ratio1).fit()\nmodel2 = smf.ols('gave ~ 1', data=df_ratio2).fit()\nmodel3 = smf.ols('gave ~ 1', data=df_ratio3).fit()\n\nprint(\"Ratio 1:1 mean gave =\", model1.params.iloc[0])\nprint(\"Ratio 2:1 mean gave =\", model2.params.iloc[0])\nprint(\"Ratio 3:1 mean gave =\", model3.params.iloc[0])\n\nRatio 1:1 mean gave = 0.0207491242252762\nRatio 2:1 mean gave = 0.022633375246991206\nRatio 3:1 mean gave = 0.02273339922724415\n\n\nI estimated three separate regressions, each restricted to a different match ratio group within the treatment population. Each model estimates the average probability of donating (gave) under the respective match ratio.\n\nUnder a 1:1 match, the donation rate was approximately 2.07%\nUnder a 2:1 match, the rate was 2.26%\nUnder a 3:1 match, the rate was 2.27%\n\nAlthough there are small numerical differences, they are not statistically significant, as confirmed by the earlier t-tests. These findings support the authors’ conclusion that increasing the match ratio beyond 1:1 does not meaningfully increase the likelihood of giving.\n\nmean_ratio1 = model1.params.iloc[0]\nmean_ratio2 = model2.params.iloc[0]\nmean_ratio3 = model3.params.iloc[0]\ndiff_21 = mean_ratio2 - mean_ratio1\ndiff_32 = mean_ratio3 - mean_ratio2\n\nprint(\"\\nDifferences from regression-based fitted coefficients:\")\nprint(\"Difference (2:1 - 1:1) =\", diff_21)\nprint(\"Difference (3:1 - 2:1) =\", diff_32)\n\ndirect_mean1 = df_ratio1['gave'].mean()\ndirect_mean2 = df_ratio2['gave'].mean()\ndirect_mean3 = df_ratio3['gave'].mean()\n\nprint(\"\\nDirectly calculated group means:\")\nprint(\"Directly calculated mean (1:1) =\", direct_mean1)\nprint(\"Directly calculated mean (2:1) =\", direct_mean2)\nprint(\"Directly calculated mean (3:1) =\", direct_mean3)\ndirect_diff_21 = direct_mean2 - direct_mean1\ndirect_diff_32 = direct_mean3 - direct_mean2\n\nprint(\"\\nDifferences computed directly from the data:\")\nprint(\"Difference (2:1 - 1:1) =\", direct_diff_21)\nprint(\"Difference (3:1 - 2:1) =\", direct_diff_32)\n\n\nDifferences from regression-based fitted coefficients:\nDifference (2:1 - 1:1) = 0.0018842510217150048\nDifference (3:1 - 2:1) = 0.00010002398025294248\n\nDirectly calculated group means:\nDirectly calculated mean (1:1) = 0.020749124225276205\nDirectly calculated mean (2:1) = 0.0226333752469912\nDirectly calculated mean (3:1) = 0.022733399227244138\n\nDifferences computed directly from the data:\nDifference (2:1 - 1:1) = 0.0018842510217149944\nDifference (3:1 - 2:1) = 0.00010002398025293902\n\n\n\n\nEffectiveness of Different Match Ratios\nTo assess whether the size of the matched donation affects response rates, I calculated the donation rate differences between the three match ratios using two approaches: (1) fitted values from separate regressions, and (2) direct group means from the data.\n\nThe response rate difference between 2:1 and 1:1 was 0.00188 (about 0.19 percentage points).\nThe response rate difference between 3:1 and 2:1 was only 0.00010 (about 0.01 percentage points).\n\nThese values are nearly identical between the regression-based and data-based methods, confirming the robustness of the findings.\n\n\nConclusion:\nWhile the 2:1 match ratio shows a slightly higher response rate than 1:1, the difference is very small, and the additional increase from 2:1 to 3:1 is almost negligible. These findings support the conclusion from the original paper that increasing the match ratio beyond 1:1 does not meaningfully enhance the likelihood of donation.\nThe implication is that once the psychological effect of a matching grant is introduced, making the match more generous (e.g., 2:1 or 3:1) offers little additional motivational benefit. The presence of a match itself is impactful — but its size does not substantially change behavior.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\nfrom scipy import stats\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf_amt = df[['amount', 'treatment']].dropna()\ntreat_amt = df_amt[df_amt['treatment'] == 1]['amount']\nctrl_amt = df_amt[df_amt['treatment'] == 0]['amount']\n\nt_stat, p_val = stats.ttest_ind(treat_amt, ctrl_amt)\nprint(\"T-test: t = {:.3f}, p = {:.4f}\".format(t_stat, p_val))\n\nT-test: t = 1.861, p = 0.0628\n\n\nTo evaluate whether offering a matching donation affects the size of contributions (i.e., the donation amount), I conducted a t-test comparing the average donation between the treatment and control groups.\nThe t-test yielded: - t = 1.861 - p = 0.0628\n\n\nInterpretation\nThe result suggests that individuals in the treatment group — those who were offered a matching donation — gave more on average than those in the control group. However, this difference is not statistically significant at the conventional 5% level, though it is marginally significant at the 10% level.\nIt appears that the presence of a match may increase donation amounts, but the effect is not strong enough to be conclusive using this method alone. The modest significance may reflect the fact that most of the increase in total fundraising comes from a higher likelihood of giving, not necessarily from larger gifts among those who give.\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf_positive = df[df['amount'] &gt; 0][['amount', 'treatment']].dropna()\nmodel = smf.ols(\"amount ~ treatment\", data=df_positive).fit()\nmodel.summary2().tables[1]\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n45.540268\n2.423378\n18.792063\n5.473578e-68\n40.784958\n50.295579\n\n\ntreatment\n-1.668393\n2.872384\n-0.580839\n5.614756e-01\n-7.304773\n3.967986\n\n\n\n\n\n\n\n\n\nInterpretation of Regression Coefficients\nThis regression analyzes how much people donate, conditional on having donated at all (amount &gt; 0). The outcome variable is the donation amount, and the independent variable is treatment (whether the donor received a matching donation offer).\n\nThe intercept (45.54) represents the average donation amount among control group donors (i.e., those who did not receive a matching offer).\nThe treatment coefficient (−1.67) tells us that, conditional on giving, donors in the treatment group gave $1.67 less on average than those in the control group.\nHowever, this difference is not statistically significant (p = 0.561), and the 95% confidence interval includes zero (from about −7.30 to +3.96). ### Does the Coefficient Have a Causal Interpretation? No, the treatment coefficient in this regression does not have a clean causal interpretation, because the sample is conditioned on the outcome (i.e., we only include people who gave). Since treatment status affects who chooses to give, the two groups may no longer be comparable. This introduces selection bias.\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf_positive = df[df['amount'] &gt; 0]\n\ncontrol_donors = df_positive[df_positive['treatment'] == 0]\ntreat_donors = df_positive[df_positive['treatment'] == 1]\n\nmean_ctrl = control_donors['amount'].mean()\nmean_treat = treat_donors['amount'].mean()\n\nsns.set(style=\"whitegrid\")\nfig, axs = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\nsns.histplot(control_donors['amount'], bins=30, ax=axs[0], color=\"#4C72B0\")\naxs[0].axvline(mean_ctrl, color='red', linestyle='--', label=f'Mean: ${mean_ctrl:.2f}')\naxs[0].set_title(\"Control Group (Donors Only)\")\naxs[0].set_xlabel(\"Donation Amount\")\naxs[0].legend()\n\nsns.histplot(treat_donors['amount'], bins=30, ax=axs[1], color=\"#55A868\")\naxs[1].axvline(mean_treat, color='red', linestyle='--', label=f'Mean: ${mean_treat:.2f}')\naxs[1].set_title(\"Treatment Group (Donors Only)\")\naxs[1].set_xlabel(\"Donation Amount\")\naxs[1].legend()\n\nplt.suptitle(\"Histogram of Donation Amounts (Among Donors)\", fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "blog/project1/hw1_questions.html#simulation-experiment",
    "href": "blog/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(42)\n\nn_simulations = 10000\nn_per_group = 100\ndiffs = []\nfor _ in range(n_simulations):\n    treat_group = np.random.binomial(1, 0.022, size=n_per_group)\n    control_group = np.random.binomial(1, 0.018, size=n_per_group)\n    \n    treat_avg = np.mean(treat_group)\n    control_avg = np.mean(control_group)\n    \n    diffs.append(treat_avg - control_avg)\ndiffs = np.array(diffs)\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n_simulations + 1)\n\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label='Cumulative Average Difference', color='blue')\nplt.axhline(0.004, color='red', linestyle='--', label='True Difference = 0.004')\nplt.title(\"Law of Large Numbers: Cumulative Average of Treatment - Control\")\nplt.xlabel(\"Number of Simulated Pairs\")\nplt.ylabel(\"Average Difference\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis simulation follows the logic of the Law of Large Numbers (LLN) by repeatedly simulating experiments with independent treatment and control groups. For each of 10,000 simulated experiments, we draw 100 observations from a control distribution (p = 0.018) and 100 from a treatment distribution (p = 0.022), calculate the difference in means, and plot the cumulative average of those differences.\nAs the number of experiments increases, the cumulative average stabilizes around the true population difference of 0.004, demonstrating that the average of estimates converges to the true value with enough data — exactly as predicted by the LLN.\n\n\nCentral Limit Theorem\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(42)\n\np_control = 0.018\np_treatment = 0.022\nn_simulations = 1000\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\naxs = axs.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    differences = []\n    for _ in range(n_simulations):\n        treat = np.random.binomial(1, p_treatment, size=n)\n        control = np.random.binomial(1, p_control, size=n)\n        diff = np.mean(treat) - np.mean(control)\n        differences.append(diff)\n    axs[i].hist(differences, bins=30, color='skyblue', edgecolor='black')\n    axs[i].axvline(0, color='red', linestyle='--', label='Zero')\n    axs[i].axvline(0.004, color='green', linestyle='--', label='True diff = 0.004')\n    axs[i].set_title(f\"Sample Size = {n}\")\n    axs[i].set_xlabel(\"Avg. Treatment - Control\")\n    axs[i].set_ylabel(\"Frequency\")\n    axs[i].legend()\n\nplt.suptitle(\"Sampling Distribution of Differences (CLT Simulation)\", fontsize=16)\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()\n\n\n\n\n\n\n\n\nThese four histograms show the distribution of the average donation rate differences between treatment and control groups over 1,000 simulated experiments at sample sizes of 50, 200, 500, and 1000. In each experiment, we randomly drew observations from two Bernoulli distributions (p=0.022 for treatment and p=0.018 for control) and computed the average difference.\n\nSample Size = 50:\nThe distribution is very wide and highly variable. Zero lies well within the bulk of the distribution, meaning it’s not uncommon to observe sample differences near or even below zero due to noise. This reflects high sampling variability with small samples.\nSample Size = 200:\nThe distribution tightens and begins to center more clearly around the true difference of 0.004. Zero is still relatively close to the middle, but it’s less dominant compared to sample size 50.\nSample Size = 500:\nThe distribution is visibly more concentrated. The peak of the histogram is now clearly to the right of zero, closer to 0.004. Zero is moving toward the tail, indicating increasing power to detect the true effect.\nSample Size = 1000:\nAt this sample size, the distribution becomes even more narrow and sharply peaked around the true mean difference. Zero now lies clearly in the tail of the distribution. This means that if the true difference is 0.004, it is increasingly unlikely (with larger samples) to observe a result as small as zero just by chance."
  },
  {
    "objectID": "blog/project2/hw2_questions.html",
    "href": "blog/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"blueprinty.csv\")\n\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\ndf.shape\n\n(1500, 4)\n\n\n\ndf.dtypes\n\npatents         int64\nregion         object\nage           float64\niscustomer      int64\ndtype: object\n\n\n\ndf.isnull().sum()\n\npatents       0\nregion        0\nage           0\niscustomer    0\ndtype: int64\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\nsns.histplot(df[df[\"iscustomer\"] == 0][\"patents\"], bins=15, color=\"gray\", ax=axs[0])\naxs[0].set_title(\"Non-Customers\")\naxs[0].set_xlabel(\"Number of Patents\")\naxs[0].set_ylabel(\"Frequency\")\n\nsns.histplot(df[df[\"iscustomer\"] == 1][\"patents\"], bins=15, color=\"green\", ax=axs[1])\naxs[1].set_title(\"Customers\")\naxs[1].set_xlabel(\"Number of Patents\")\n\nplt.suptitle(\"Patent Distributions by Customer Status\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf.groupby(\"iscustomer\")[\"patents\"].mean()\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\nThe histograms and average values reveal the following:\n\nNon-Customers (iscustomer = 0):\n\nPatent counts are left-skewed, concentrated around 0–5 patents.\nThe average number of patents is 3.47.\n\nCustomers (iscustomer = 1):\n\nThe distribution is slightly right-shifted, with more firms having 3–6 patents.\nThe average number of patents is 4.13.\n\n\n\n\nOn average, firms that use Blueprinty’s software hold more patents than those that do not.\nThis difference in distribution and mean provides initial evidence that Blueprinty customers may be more successful in securing patents.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nsns.histplot(data=df, x=\"age\", hue=\"iscustomer\", element=\"step\", stat=\"density\", common_norm=False)\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Firm Age\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Customer\", labels=[\"Customer\", \"Non-Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf.groupby(\"iscustomer\")[\"age\"].mean()\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nsns.histplot(data=df, x=\"region\", hue=\"iscustomer\", element=\"step\", stat=\"density\", common_norm=False)\nplt.title(\"region Distribution by Customer Status\")\nplt.xlabel(\"region\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Customer\", labels=[\"Customer\", \"Non-Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\npd.crosstab(df[\"region\"], df[\"iscustomer\"], normalize='columns')\n\n\n\n\n\n\n\niscustomer\n0\n1\n\n\nregion\n\n\n\n\n\n\nMidwest\n0.183513\n0.076923\n\n\nNortheast\n0.267910\n0.681913\n\n\nNorthwest\n0.155054\n0.060291\n\n\nSouth\n0.153091\n0.072765\n\n\nSouthwest\n0.240432\n0.108108\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe age distribution plot shows a slight shift: Blueprinty customers are slightly younger on average.\nGroup means confirm this:\n\nNon-customers: 26.10 years\nCustomers: 26.90 years\n\nWhile the difference in means is small (~0.8 years), the distribution also suggests customers may be a bit more concentrated in the 20–30 age range, while non-customers have a longer right tail (i.e., more older firms).\n\n\n\n\n\nThe regional histogram and normalized cross-tab show strong regional skew:\n\nAmong customers, 68% are from the Northeast, compared to only ~27% of non-customers.\nThe Southwest and Northwest have noticeably fewer customers compared to non-customers.\nThis indicates that Blueprinty customers are disproportionately concentrated in the Northeast, whereas non-customers are more evenly spread across regions.\n\n\nThese results highlight systematic differences between customers and non-customers:\n\nCustomers tend to be slightly younger.\nCustomers are heavily concentrated in the Northeast region.\n\nThese imbalances suggest that age and region may confound the observed relationship between Blueprinty usage and patent success.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWrite down mathematically the likelihood for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nIf we observe \\(n\\) independent observations \\(Y_1, Y_2, \\dots, Y_n\\) from a Poisson distribution, the likelihood function is:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} f(Y_i|\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n= e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^{n} \\frac{1}{Y_i!}\n\\]\nTaking the natural log of the likelihood, we obtain the log-likelihood function:\n\\[\n\\ell(\\lambda) = \\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^n Y_i \\right) \\log \\lambda - \\sum_{i=1}^n \\log(Y_i!)\n\\]\nTo find the maximum likelihood estimator (MLE), we take the derivative with respect to \\(\\lambda\\), set it equal to 0, and solve:\n\\[\n\\frac{d\\ell}{d\\lambda} = -n + \\frac{\\sum Y_i}{\\lambda} = 0 \\quad \\Rightarrow \\quad \\hat{\\lambda} = \\frac{1}{n} \\sum Y_i\n\\]\nTherefore, the MLE of \\(\\lambda\\) is the sample mean \\(\\bar{Y}\\).\n\nimport numpy as np\nfrom scipy.special import gammaln \n\ndef poisson_loglikelihood(lmbda, Y):\n\n    if lmbda &lt;= 0:\n        return -np.inf  \n    \n    Y = np.array(Y)\n    log_likelihood = np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n    return log_likelihood\n\nWe used our log-likelihood function to evaluate the Poisson model over a range of λ values, using the observed number of patents as our data.\nThe plot above shows that the log-likelihood is maximized near the sample mean of the data (shown by the red dashed line), which is consistent with the fact that the MLE for λ in a Poisson model is simply the sample mean.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nY = df[\"patents\"]\n\nlambda_vals = np.linspace(0.1, 20, 200)\n\nlog_likes = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]\n\nplt.figure(figsize=(10, 5))\nplt.plot(lambda_vals, log_likes, color='blue')\nplt.axvline(Y.mean(), color='red', linestyle='--', label=f'MLE (mean of Y) = {Y.mean():.2f}')\nplt.title(\"Log-Likelihood of Poisson Model\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nRecall that the log-likelihood for the Poisson model is:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left[ -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right]\n\\]\nTaking the derivative with respect to ( ):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^{n} \\left[ -1 + \\frac{Y_i}{\\lambda} \\right]\n= -n + \\frac{\\sum Y_i}{\\lambda}\n\\]\nSetting the derivative equal to zero:\n\\[\n-n + \\frac{\\sum Y_i}{\\lambda} = 0\n\\quad \\Rightarrow \\quad\n\\hat{\\lambda} = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nTherefore, the maximum likelihood estimator ( ) is the sample mean.\nOptimizing my likelihood function with sp.optimize() in Python.\n\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\n\ndef neg_poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return np.inf\n    return -np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\nY = df[\"patents\"]\n\nresult = minimize_scalar(neg_poisson_loglikelihood, bounds=(0.01, 20), args=(Y,), method='bounded')\n\nprint(f\"Numerical MLE for lambda: {result.x:.4f}\")\nprint(f\"Sample mean of Y (closed-form MLE): {Y.mean():.4f}\")\n\nNumerical MLE for lambda: 3.6847\nSample mean of Y (closed-form MLE): 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta)\n    Y = np.asarray(Y)\n    X = np.asarray(X)\n\n    lambda_i = np.exp(X @ beta)\n\n    log_likelihood = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n\n    return log_likelihood\n\n\nimport numpy as np\nimport pandas as pd\nimport patsy\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\n\ndf[\"age2\"] = df[\"age\"] ** 2\ndesign = patsy.dmatrix(\"1 + age + age2 + C(region) + iscustomer\",\n                       df, return_type=\"dataframe\")\nX = design.values       \nY = df[\"patents\"].values\nnames = design.design_info.column_names\n\ndef loglik(beta, Y, X):\n    λ = np.exp(X @ beta)\n    return np.sum(-λ + Y * np.log(λ) - gammaln(Y + 1))\n\ndef negloglik(beta, Y, X):\n    return -loglik(beta, Y, X)\n\ndef grad_negloglik(beta, Y, X):\n    λ = np.exp(X @ beta)\n    return X.T.dot(λ - Y)\n\ndef hess_negloglik(beta, Y, X):\n    λ = np.exp(X @ beta)\n    return X.T.dot(X * λ[:, None])\nbeta0 = np.zeros(X.shape[1])\n\nres = minimize(\n    fun=negloglik,\n    x0=beta0,\n    args=(Y, X),\n    method='trust-ncg',      \n    jac=grad_negloglik,\n    hess=hess_negloglik,\n    options={'gtol':1e-8, 'xtol':1e-8, 'maxiter':100, 'disp': True}\n)\nbeta_hat = res.x\n\nH_obs   = hess_negloglik(beta_hat, Y, X)\ncov_beta= np.linalg.inv(H_obs)\nse_beta = np.sqrt(np.diag(cov_beta))\n\nmle = pd.DataFrame({\n    \"Coefficient (MLE)\": beta_hat,\n    \"Std. Error (MLE)\": se_beta\n}, index=names).round(4)\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nglm = smf.glm(\n    formula=\"patents ~ age + age2 + C(region) + iscustomer\",\n    data=df,\n    family=sm.families.Poisson()\n).fit()\n\nglm_res = pd.DataFrame({\n    \"Coefficient (GLM)\": glm.params,\n    \"Std. Error (GLM)\": glm.bse\n}).round(4)\n\nprint(mle.join(glm_res))\n\n         Current function value: 3258.072145\n         Iterations: 39\n         Function evaluations: 37\n         Gradient evaluations: 35\n         Hessian evaluations: 35\n\n\nC:\\Users\\25645\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:732: OptimizeWarning:\n\nUnknown solver options: xtol\n\nC:\\Users\\25645\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:732: RuntimeWarning:\n\nA bad approximation caused failure to predict improvement.\n\n\n\n                        Coefficient (MLE)  Std. Error (MLE)  \\\nIntercept                         -0.5089            0.1832   \nC(region)[T.Northeast]             0.0292            0.0436   \nC(region)[T.Northwest]            -0.0176            0.0538   \nC(region)[T.South]                 0.0566            0.0527   \nC(region)[T.Southwest]             0.0506            0.0472   \nage                                0.1486            0.0139   \nage2                              -0.0030            0.0003   \niscustomer                         0.2076            0.0309   \n\n                        Coefficient (GLM)  Std. Error (GLM)  \nIntercept                         -0.5089            0.1832  \nC(region)[T.Northeast]             0.0292            0.0436  \nC(region)[T.Northwest]            -0.0176            0.0538  \nC(region)[T.South]                 0.0566            0.0527  \nC(region)[T.Southwest]             0.0506            0.0472  \nage                                0.1486            0.0139  \nage2                              -0.0030            0.0003  \niscustomer                         0.2076            0.0309  \n\n\n\n\n\nWe fit a Poisson regression model to predict the number of patents awarded to engineering firms using the following predictors: - Firm age and age squared - Region (Midwest is the omitted reference group) - Whether the firm is a Blueprinty customer\nThe results from both our custom MLE implementation and the statsmodels GLM function are identical, confirming the correctness of our likelihood function and optimization procedure.\n\n\n\nIntercept: The baseline log-rate of patents for a firm in the Midwest, with average age, and not using Blueprinty.\nRegion Effects: None of the region coefficients are statistically significant at conventional levels. This suggests no strong evidence that region alone explains differences in patenting rates, relative to the Midwest baseline.\nAge and Age²:\n\nThe coefficient on age is positive and significant, while age² is negative and significant.\nThis implies a concave (inverted U-shaped) relationship between age and patenting: younger firms see increasing patenting with age, but the rate of growth slows and eventually declines for older firms.\n\nBlueprinty Customer (iscustomer):\n\nThe coefficient is positive (0.2076) and statistically significant (p &lt; 0.01).\nInterpreted in the log-linear Poisson context, being a customer is associated with an expected increase in patent rate by a factor of ( e^{0.2076} ), or a 23% higher rate of patenting, holding other variables constant.\n\n\n\nimport numpy as np\n\nX_0 = X.copy()\nX_1 = X.copy()\n\niscustomer_index = names.index(\"iscustomer\")\n\nX_0[:, iscustomer_index] = 0\nX_1[:, iscustomer_index] = 1\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\navg_effect = np.mean(y_pred_1 - y_pred_0)\n\nprint(f\"Average predicted effect of Blueprinty software: {avg_effect:.4f} patents\")\n\nAverage predicted effect of Blueprinty software: 0.7928 patents"
  },
  {
    "objectID": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"blueprinty.csv\")\n\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\ndf.shape\n\n(1500, 4)\n\n\n\ndf.dtypes\n\npatents         int64\nregion         object\nage           float64\niscustomer      int64\ndtype: object\n\n\n\ndf.isnull().sum()\n\npatents       0\nregion        0\nage           0\niscustomer    0\ndtype: int64\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\nsns.histplot(df[df[\"iscustomer\"] == 0][\"patents\"], bins=15, color=\"gray\", ax=axs[0])\naxs[0].set_title(\"Non-Customers\")\naxs[0].set_xlabel(\"Number of Patents\")\naxs[0].set_ylabel(\"Frequency\")\n\nsns.histplot(df[df[\"iscustomer\"] == 1][\"patents\"], bins=15, color=\"green\", ax=axs[1])\naxs[1].set_title(\"Customers\")\naxs[1].set_xlabel(\"Number of Patents\")\n\nplt.suptitle(\"Patent Distributions by Customer Status\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf.groupby(\"iscustomer\")[\"patents\"].mean()\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\nThe histograms and average values reveal the following:\n\nNon-Customers (iscustomer = 0):\n\nPatent counts are left-skewed, concentrated around 0–5 patents.\nThe average number of patents is 3.47.\n\nCustomers (iscustomer = 1):\n\nThe distribution is slightly right-shifted, with more firms having 3–6 patents.\nThe average number of patents is 4.13.\n\n\n\n\nOn average, firms that use Blueprinty’s software hold more patents than those that do not.\nThis difference in distribution and mean provides initial evidence that Blueprinty customers may be more successful in securing patents.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nsns.histplot(data=df, x=\"age\", hue=\"iscustomer\", element=\"step\", stat=\"density\", common_norm=False)\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Firm Age\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Customer\", labels=[\"Customer\", \"Non-Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf.groupby(\"iscustomer\")[\"age\"].mean()\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nsns.histplot(data=df, x=\"region\", hue=\"iscustomer\", element=\"step\", stat=\"density\", common_norm=False)\nplt.title(\"region Distribution by Customer Status\")\nplt.xlabel(\"region\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Customer\", labels=[\"Customer\", \"Non-Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\npd.crosstab(df[\"region\"], df[\"iscustomer\"], normalize='columns')\n\n\n\n\n\n\n\niscustomer\n0\n1\n\n\nregion\n\n\n\n\n\n\nMidwest\n0.183513\n0.076923\n\n\nNortheast\n0.267910\n0.681913\n\n\nNorthwest\n0.155054\n0.060291\n\n\nSouth\n0.153091\n0.072765\n\n\nSouthwest\n0.240432\n0.108108\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe age distribution plot shows a slight shift: Blueprinty customers are slightly younger on average.\nGroup means confirm this:\n\nNon-customers: 26.10 years\nCustomers: 26.90 years\n\nWhile the difference in means is small (~0.8 years), the distribution also suggests customers may be a bit more concentrated in the 20–30 age range, while non-customers have a longer right tail (i.e., more older firms).\n\n\n\n\n\nThe regional histogram and normalized cross-tab show strong regional skew:\n\nAmong customers, 68% are from the Northeast, compared to only ~27% of non-customers.\nThe Southwest and Northwest have noticeably fewer customers compared to non-customers.\nThis indicates that Blueprinty customers are disproportionately concentrated in the Northeast, whereas non-customers are more evenly spread across regions.\n\n\nThese results highlight systematic differences between customers and non-customers:\n\nCustomers tend to be slightly younger.\nCustomers are heavily concentrated in the Northeast region.\n\nThese imbalances suggest that age and region may confound the observed relationship between Blueprinty usage and patent success.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWrite down mathematically the likelihood for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\nIf we observe \\(n\\) independent observations \\(Y_1, Y_2, \\dots, Y_n\\) from a Poisson distribution, the likelihood function is:\n\\[\nL(\\lambda) = \\prod_{i=1}^{n} f(Y_i|\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n= e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^{n} \\frac{1}{Y_i!}\n\\]\nTaking the natural log of the likelihood, we obtain the log-likelihood function:\n\\[\n\\ell(\\lambda) = \\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^n Y_i \\right) \\log \\lambda - \\sum_{i=1}^n \\log(Y_i!)\n\\]\nTo find the maximum likelihood estimator (MLE), we take the derivative with respect to \\(\\lambda\\), set it equal to 0, and solve:\n\\[\n\\frac{d\\ell}{d\\lambda} = -n + \\frac{\\sum Y_i}{\\lambda} = 0 \\quad \\Rightarrow \\quad \\hat{\\lambda} = \\frac{1}{n} \\sum Y_i\n\\]\nTherefore, the MLE of \\(\\lambda\\) is the sample mean \\(\\bar{Y}\\).\n\nimport numpy as np\nfrom scipy.special import gammaln \n\ndef poisson_loglikelihood(lmbda, Y):\n\n    if lmbda &lt;= 0:\n        return -np.inf  \n    \n    Y = np.array(Y)\n    log_likelihood = np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n    return log_likelihood\n\nWe used our log-likelihood function to evaluate the Poisson model over a range of λ values, using the observed number of patents as our data.\nThe plot above shows that the log-likelihood is maximized near the sample mean of the data (shown by the red dashed line), which is consistent with the fact that the MLE for λ in a Poisson model is simply the sample mean.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nY = df[\"patents\"]\n\nlambda_vals = np.linspace(0.1, 20, 200)\n\nlog_likes = [poisson_loglikelihood(lmbda, Y) for lmbda in lambda_vals]\n\nplt.figure(figsize=(10, 5))\nplt.plot(lambda_vals, log_likes, color='blue')\nplt.axvline(Y.mean(), color='red', linestyle='--', label=f'MLE (mean of Y) = {Y.mean():.2f}')\nplt.title(\"Log-Likelihood of Poisson Model\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nRecall that the log-likelihood for the Poisson model is:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^{n} \\left[ -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right]\n\\]\nTaking the derivative with respect to ( ):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^{n} \\left[ -1 + \\frac{Y_i}{\\lambda} \\right]\n= -n + \\frac{\\sum Y_i}{\\lambda}\n\\]\nSetting the derivative equal to zero:\n\\[\n-n + \\frac{\\sum Y_i}{\\lambda} = 0\n\\quad \\Rightarrow \\quad\n\\hat{\\lambda} = \\frac{1}{n} \\sum Y_i = \\bar{Y}\n\\]\nTherefore, the maximum likelihood estimator ( ) is the sample mean.\nOptimizing my likelihood function with sp.optimize() in Python.\n\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\n\ndef neg_poisson_loglikelihood(lmbda, Y):\n    if lmbda &lt;= 0:\n        return np.inf\n    return -np.sum(-lmbda + Y * np.log(lmbda) - gammaln(Y + 1))\n\nY = df[\"patents\"]\n\nresult = minimize_scalar(neg_poisson_loglikelihood, bounds=(0.01, 20), args=(Y,), method='bounded')\n\nprint(f\"Numerical MLE for lambda: {result.x:.4f}\")\nprint(f\"Sample mean of Y (closed-form MLE): {Y.mean():.4f}\")\n\nNumerical MLE for lambda: 3.6847\nSample mean of Y (closed-form MLE): 3.6847\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta)\n    Y = np.asarray(Y)\n    X = np.asarray(X)\n\n    lambda_i = np.exp(X @ beta)\n\n    log_likelihood = np.sum(-lambda_i + Y * np.log(lambda_i) - gammaln(Y + 1))\n\n    return log_likelihood\n\n\nimport numpy as np\nimport pandas as pd\nimport patsy\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\n\ndf[\"age2\"] = df[\"age\"] ** 2\ndesign = patsy.dmatrix(\"1 + age + age2 + C(region) + iscustomer\",\n                       df, return_type=\"dataframe\")\nX = design.values       \nY = df[\"patents\"].values\nnames = design.design_info.column_names\n\ndef loglik(beta, Y, X):\n    λ = np.exp(X @ beta)\n    return np.sum(-λ + Y * np.log(λ) - gammaln(Y + 1))\n\ndef negloglik(beta, Y, X):\n    return -loglik(beta, Y, X)\n\ndef grad_negloglik(beta, Y, X):\n    λ = np.exp(X @ beta)\n    return X.T.dot(λ - Y)\n\ndef hess_negloglik(beta, Y, X):\n    λ = np.exp(X @ beta)\n    return X.T.dot(X * λ[:, None])\nbeta0 = np.zeros(X.shape[1])\n\nres = minimize(\n    fun=negloglik,\n    x0=beta0,\n    args=(Y, X),\n    method='trust-ncg',      \n    jac=grad_negloglik,\n    hess=hess_negloglik,\n    options={'gtol':1e-8, 'xtol':1e-8, 'maxiter':100, 'disp': True}\n)\nbeta_hat = res.x\n\nH_obs   = hess_negloglik(beta_hat, Y, X)\ncov_beta= np.linalg.inv(H_obs)\nse_beta = np.sqrt(np.diag(cov_beta))\n\nmle = pd.DataFrame({\n    \"Coefficient (MLE)\": beta_hat,\n    \"Std. Error (MLE)\": se_beta\n}, index=names).round(4)\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nglm = smf.glm(\n    formula=\"patents ~ age + age2 + C(region) + iscustomer\",\n    data=df,\n    family=sm.families.Poisson()\n).fit()\n\nglm_res = pd.DataFrame({\n    \"Coefficient (GLM)\": glm.params,\n    \"Std. Error (GLM)\": glm.bse\n}).round(4)\n\nprint(mle.join(glm_res))\n\n         Current function value: 3258.072145\n         Iterations: 39\n         Function evaluations: 37\n         Gradient evaluations: 35\n         Hessian evaluations: 35\n\n\nC:\\Users\\25645\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:732: OptimizeWarning:\n\nUnknown solver options: xtol\n\nC:\\Users\\25645\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:732: RuntimeWarning:\n\nA bad approximation caused failure to predict improvement.\n\n\n\n                        Coefficient (MLE)  Std. Error (MLE)  \\\nIntercept                         -0.5089            0.1832   \nC(region)[T.Northeast]             0.0292            0.0436   \nC(region)[T.Northwest]            -0.0176            0.0538   \nC(region)[T.South]                 0.0566            0.0527   \nC(region)[T.Southwest]             0.0506            0.0472   \nage                                0.1486            0.0139   \nage2                              -0.0030            0.0003   \niscustomer                         0.2076            0.0309   \n\n                        Coefficient (GLM)  Std. Error (GLM)  \nIntercept                         -0.5089            0.1832  \nC(region)[T.Northeast]             0.0292            0.0436  \nC(region)[T.Northwest]            -0.0176            0.0538  \nC(region)[T.South]                 0.0566            0.0527  \nC(region)[T.Southwest]             0.0506            0.0472  \nage                                0.1486            0.0139  \nage2                              -0.0030            0.0003  \niscustomer                         0.2076            0.0309  \n\n\n\n\n\nWe fit a Poisson regression model to predict the number of patents awarded to engineering firms using the following predictors: - Firm age and age squared - Region (Midwest is the omitted reference group) - Whether the firm is a Blueprinty customer\nThe results from both our custom MLE implementation and the statsmodels GLM function are identical, confirming the correctness of our likelihood function and optimization procedure.\n\n\n\nIntercept: The baseline log-rate of patents for a firm in the Midwest, with average age, and not using Blueprinty.\nRegion Effects: None of the region coefficients are statistically significant at conventional levels. This suggests no strong evidence that region alone explains differences in patenting rates, relative to the Midwest baseline.\nAge and Age²:\n\nThe coefficient on age is positive and significant, while age² is negative and significant.\nThis implies a concave (inverted U-shaped) relationship between age and patenting: younger firms see increasing patenting with age, but the rate of growth slows and eventually declines for older firms.\n\nBlueprinty Customer (iscustomer):\n\nThe coefficient is positive (0.2076) and statistically significant (p &lt; 0.01).\nInterpreted in the log-linear Poisson context, being a customer is associated with an expected increase in patent rate by a factor of ( e^{0.2076} ), or a 23% higher rate of patenting, holding other variables constant.\n\n\n\nimport numpy as np\n\nX_0 = X.copy()\nX_1 = X.copy()\n\niscustomer_index = names.index(\"iscustomer\")\n\nX_0[:, iscustomer_index] = 0\nX_1[:, iscustomer_index] = 1\ny_pred_0 = np.exp(X_0 @ beta_hat)\ny_pred_1 = np.exp(X_1 @ beta_hat)\n\navg_effect = np.mean(y_pred_1 - y_pred_0)\n\nprint(f\"Average predicted effect of Blueprinty software: {avg_effect:.4f} patents\")\n\nAverage predicted effect of Blueprinty software: 0.7928 patents"
  },
  {
    "objectID": "blog/project2/hw2_questions.html#airbnb-case-study",
    "href": "blog/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nimport pandas as pd\nAB = pd.read_csv(\"airbnb.csv\")\nAB.info()\nAB.describe(include='all')\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 40628 entries, 0 to 40627\nData columns (total 14 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   Unnamed: 0                 40628 non-null  int64  \n 1   id                         40628 non-null  int64  \n 2   days                       40628 non-null  int64  \n 3   last_scraped               40628 non-null  object \n 4   host_since                 40593 non-null  object \n 5   room_type                  40628 non-null  object \n 6   bathrooms                  40468 non-null  float64\n 7   bedrooms                   40552 non-null  float64\n 8   price                      40628 non-null  int64  \n 9   number_of_reviews          40628 non-null  int64  \n 10  review_scores_cleanliness  30433 non-null  float64\n 11  review_scores_location     30374 non-null  float64\n 12  review_scores_value        30372 non-null  float64\n 13  instant_bookable           40628 non-null  object \ndtypes: float64(5), int64(5), object(4)\nmemory usage: 4.3+ MB\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\ncount\n40628.000000\n4.062800e+04\n40628.000000\n40628\n40593\n40628\n40468.000000\n40552.000000\n40628.000000\n40628.000000\n30433.000000\n30374.000000\n30372.000000\n40628\n\n\nunique\nNaN\nNaN\nNaN\n2\n2790\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2\n\n\ntop\nNaN\nNaN\nNaN\n4/2/2017\n12/21/2015\nEntire home/apt\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nf\n\n\nfreq\nNaN\nNaN\nNaN\n25737\n65\n19873\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n32759\n\n\nmean\n20314.500000\n9.698889e+06\n1102.368219\nNaN\nNaN\nNaN\n1.124592\n1.147046\n144.760732\n15.904426\n9.198370\n9.413544\n9.331522\nNaN\n\n\nstd\n11728.437705\n5.460166e+06\n1383.269358\nNaN\nNaN\nNaN\n0.385884\n0.691746\n210.657597\n29.246009\n1.119935\n0.844949\n0.902966\nNaN\n\n\nmin\n1.000000\n2.515000e+03\n1.000000\nNaN\nNaN\nNaN\n0.000000\n0.000000\n10.000000\n0.000000\n2.000000\n2.000000\n2.000000\nNaN\n\n\n25%\n10157.750000\n4.889868e+06\n542.000000\nNaN\nNaN\nNaN\n1.000000\n1.000000\n70.000000\n1.000000\n9.000000\n9.000000\n9.000000\nNaN\n\n\n50%\n20314.500000\n9.862878e+06\n996.000000\nNaN\nNaN\nNaN\n1.000000\n1.000000\n100.000000\n4.000000\n10.000000\n10.000000\n10.000000\nNaN\n\n\n75%\n30471.250000\n1.466789e+07\n1535.000000\nNaN\nNaN\nNaN\n1.000000\n1.000000\n170.000000\n17.000000\n10.000000\n10.000000\n10.000000\nNaN\n\n\nmax\n40628.000000\n1.800967e+07\n42828.000000\nNaN\nNaN\nNaN\n8.000000\n10.000000\n10000.000000\n421.000000\n10.000000\n10.000000\n10.000000\nNaN\n\n\n\n\n\n\n\n\nrelevant_vars = [\n    \"number_of_reviews\", \"days\", \"room_type\", \"bathrooms\", \"bedrooms\",\n    \"price\", \"review_scores_cleanliness\", \"review_scores_location\",\n    \"review_scores_value\", \"instant_bookable\"\n]\n\nAB_clean = AB[relevant_vars].dropna()\n\nAB_clean[\"instant_bookable\"] = AB_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.histplot(AB_clean[\"number_of_reviews\"], bins=50)\nplt.title(\"Distribution of Number of Reviews\")\nplt.show()\n\nsns.boxplot(x=\"room_type\", y=\"number_of_reviews\", data=AB_clean)\nplt.title(\"Reviews by Room Type\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport patsy\nimport statsmodels.api as sm\nformula = \"number_of_reviews ~ days + bathrooms + bedrooms + price + review_scores_cleanliness + review_scores_location + review_scores_value + C(room_type) + instant_bookable\"\ny, X = patsy.dmatrices(formula, AB_clean, return_type=\"dataframe\")\n\npoisson_model = sm.GLM(y, X, family=sm.families.Poisson()).fit()\nsummary_df = pd.DataFrame({\n    \"Coefficient\": poisson_model.params,\n    \"Std. Error\": poisson_model.bse\n}).round(4)\n\nsummary_df\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nIntercept\n3.4980\n0.0161\n\n\nC(room_type)[T.Private room]\n-0.0105\n0.0027\n\n\nC(room_type)[T.Shared room]\n-0.2463\n0.0086\n\n\ndays\n0.0001\n0.0000\n\n\nbathrooms\n-0.1177\n0.0037\n\n\nbedrooms\n0.0741\n0.0020\n\n\nprice\n-0.0000\n0.0000\n\n\nreview_scores_cleanliness\n0.1131\n0.0015\n\n\nreview_scores_location\n-0.0769\n0.0016\n\n\nreview_scores_value\n-0.0911\n0.0018\n\n\ninstant_bookable\n0.3459\n0.0029\n\n\n\n\n\n\n\n\n\nInterpretation of Poisson Regression Results\nWe used a Poisson regression model to estimate how Airbnb listing features are associated with the number of reviews, which we treat as a proxy for the number of bookings.\nBelow is an interpretation of the key coefficients:\n\nIntercept (3.498)\nThe expected log number of reviews for the baseline listing — an “Entire home/apt” with zero values for numeric variables — is 3.498. This serves as the reference point.\nRoom Type\nThe model includes two dummy variables for room type (relative to the reference category, which is likely “Entire home/apt”):\n\nC(room_type)[T.Private room] = -0.0105\n→ Slightly lower review counts, but effect is minimal.\nC(room_type)[T.Shared room] = -0.2463\n→ Listings classified as Shared Rooms receive fewer reviews. Holding other variables constant, the expected number of reviews is lower by approximately 22%:\n( e^{-0.2463} )\n\nDays Listed (0.0001)\nA small but positive coefficient, suggesting that listings that have been active longer tend to accumulate more reviews.\nBathrooms (-0.1177)\nSurprisingly, an increase in the number of bathrooms is associated with fewer reviews, all else equal. This may reflect luxury listings with less volume or niche targeting.\nBedrooms (0.0741)\nMore bedrooms are associated with more reviews — likely due to accommodating larger groups, which increases bookings.\nPrice (≈ 0)\nThe price coefficient is close to 0, suggesting that, controlling for other features, price has little to no direct relationship with review count.\nReview Scores:\n\nCleanliness (0.1131)\nHigher cleanliness scores are strongly associated with more reviews, reflecting customer satisfaction and listing quality.\nLocation (-0.0769)\nUnexpectedly negative, possibly indicating that some centrally located listings get fewer reviews per listing (could be more competitive or high turnover).\nValue (-0.0911)\nAlso negative; potentially reflects expectations mismatch for certain price-quality ratios.\n\nInstant Bookable (0.3459)\nListings that can be booked instantly receive significantly more reviews.\n( e^{0.3459} ), suggesting about a 41% increase in expected review counts for instant-bookable listings.\n\n\n\n\nSummary\nThe strongest positive predictors of review volume are: - Being instantly bookable - Higher cleanliness scores - More bedrooms\nShared rooms and higher bathroom counts are associated with fewer reviews, which may reflect listing type preferences or usage patterns."
  }
]